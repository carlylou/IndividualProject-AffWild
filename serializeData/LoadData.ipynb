{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all frames and its labels \n",
    "# 4 columns : frame number, frame, valence, arousal\n",
    "# tfrecords wrtie\n",
    "# tfrecords read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read all frames and its labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = '/vol/bitbucket/ml9915/validation/validationVideos/'\n",
    "annotationPath = '/vol/bitbucket/ml9915/validation/validationLabel/'\n",
    "tfRecordsPath = '/vol/bitbucket/ml9915/TFRecords/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnnotation(filePath):\n",
    "    file = open(filePath, 'r') \n",
    "    data = []\n",
    "    for line in file: \n",
    "        data.append(line.split())\n",
    "    array = np.array(data)\n",
    "    array = array.astype(np.float)# since the data are frame number which is int and valence arousal are range from -1000 to 1000\n",
    "    array = array.astype(np.int)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(addr):\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    img = cv2.imread(addr)\n",
    "#     img = cv2.resize(img, (224, 224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoNames(annotationPath):\n",
    "    files = os.listdir(annotationPath)\n",
    "    drop = filter(lambda x: x.startswith('.'), files)\n",
    "    for item in drop:\n",
    "        files.remove(item)\n",
    "    videoNames = []\n",
    "    for item in files:\n",
    "        videoNames.append(item.split('.')[0])\n",
    "    videoNames = sorted(videoNames)\n",
    "    return videoNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Records features\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 1-30-1280x720\n",
      "finish 107-30-640x480\n",
      "finish 112-30-640x360\n",
      "finish 113-60-1280x720\n",
      "finish 114-30-1280x720-1\n",
      "finish 114-30-1280x720-2\n",
      "finish 115-30-1280x720\n",
      "finish 118-30-640x480\n"
     ]
    }
   ],
   "source": [
    "videoNames = getVideoNames(annotationPath)\n",
    "for videoName in videoNames:\n",
    "    # get labels\n",
    "    labelFile = annotationPath+videoName+'.txt'\n",
    "    # frameNo, Valence, Arousal in one video\n",
    "    frameVA = getAnnotation(labelFile)\n",
    "    tfRecordFilePath = tfRecordsPath+videoName+'.tfrecords'\n",
    "    framesPath = videoPath+videoName+'/'\n",
    "    # open the TFRecords file\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordFilePath)\n",
    "    for record in frameVA:\n",
    "        name = '{0:05d}'.format(record[0])\n",
    "        img = load_image(framesPath+name+'.jpg')\n",
    "        # Create a feature\n",
    "        feature = {\n",
    "                        'frameNo': _bytes_feature(tf.compat.as_bytes(videoName+'/'+name)),\n",
    "                       'frame': _bytes_feature(tf.compat.as_bytes(img.tostring())),\n",
    "                      'valence': _int64_feature(record[1]),\n",
    "                      'arousal': _int64_feature(record[2])\n",
    "                  }\n",
    "        # Create an example protocol buffer\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        # Serialize to string and write on the file\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    print 'finish ' + videoName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avenv2.7",
   "language": "python",
   "name": "venv-2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
